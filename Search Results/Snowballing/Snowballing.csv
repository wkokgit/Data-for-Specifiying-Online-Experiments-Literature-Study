"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"MX3L4MAV","conferencePaper","2014","Bakshy, Eytan; Eckles, Dean; Bernstein, Michael S.","Designing and deploying online field experiments","Proceedings of the 23rd international conference on World wide web","978-1-4503-2744-2","","10.1145/2566486.2567967","https://doi.org/10.1145/2566486.2567967","Online experiments are widely used to compare specific design alternatives, but they can also be used to produce generalizable knowledge and inform strategic decision making. Doing so often requires sophisticated experimental designs, iterative refinement, and careful logging and analysis. Few tools exist that support these needs. We thus introduce a language for online field experiments called PlanOut. PlanOut separates experimental design from application code, allowing the experimenter to concisely describe experimental designs, whether common ""A/B tests"" and factorial designs, or more complex designs involving conditional logic or multiple experimental units. These latter designs are often useful for understanding causal mechanisms involved in user behaviors. We demonstrate how experiments from the literature can be implemented in PlanOut, and describe two large field experiments conducted on Facebook with PlanOut. For common scenarios in which experiments are run iteratively and in parallel, we introduce a namespaced management system that encourages sound experimental practice.","2014-04-07","2021-01-25 13:36:44","2021-01-25 13:36:44","2021-01-25","283–292","","","","","","","WWW '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","C:\Users\wouterkok\Zotero\storage\FK6MZBNA\Bakshy et al. - 2014 - Designing and deploying online field experiments.pdf","","","A/B testing; methodology; online experiments; toolkits","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PHMRDFNV","conferencePaper","2007","Kohavi, Ron; Henne, Randal M.; Sommerfield, Dan","Practical guide to controlled experiments on the web: listen to your customers not to the hippo","Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining","978-1-59593-609-7","","10.1145/1281192.1281295","http://doi.org/10.1145/1281192.1281295","The web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments, also called randomized experiments (single factor or factorial designs), A/B tests (and their generalizations), split tests, Control/Treatment tests, and parallel flights. Controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior. We provide a practical guide to conducting online experiments, where end-users can help guide the development of features. Our experience indicates that significant learning and return-on-investment (ROI) are seen when development teams listen to their customers, not to the Highest Paid Person's Opinion (HiPPO). We provide several examples of controlled experiments with surprising results. We review the important ingredients of running controlled experiments, and discuss their limitations (both technical and organizational). We focus on several areas that are critical to experimentation, including statistical power, sample size, and techniques for variance reduction. We describe common architectures for experimentation systems and analyze their advantages and disadvantages. We evaluate randomization and hashing techniques, which we show are not as simple in practice as is often assumed. Controlled experiments typically generate large amounts of data, which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest, leading to new hypotheses and creating a virtuous cycle of improvements. Organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses. Based on our extensive practical experience with multiple systems and organizations, we share key lessons that will help practitioners in running trustworthy controlled experiments.","2007-08-12","2021-01-25 14:25:42","2021-01-25 14:25:42","2021-01-25","959–967","","","","","","Practical guide to controlled experiments on the web","KDD '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","C:\Users\wouterkok\Zotero\storage\5S2FSPZA\Kohavi et al. - 2007 - Practical guide to controlled experiments on the w.pdf","","","A/B testing; controlled experiments; e-commerce","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""