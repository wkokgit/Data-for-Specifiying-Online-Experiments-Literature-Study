"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"96UPQHMH","conferencePaper","2018","Fabijan, A.; Dmitriev, P.; Olsson, H. Holmstrom; Bosch, J.","Online Controlled Experimentation at Scale: An Empirical Survey on the Current State of A/B Testing","2018 44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)","","","10.1109/SEAA.2018.00021","","Online Controlled Experiments (OCEs, aka A/B tests) are one of the most powerful methods for measuring how much value new features and changes deployed to software products bring to users. Companies like Microsoft, Amazon, and Booking.com report the ability to conduct thousands of OCEs every year. However, the competences of the remainder of the online software industry remain unknown. The main objective of this paper is to reveal the current state of A/B testing maturity in the software industry based on a maturity model from our previous research. We base our findings on 44 responses from an online empirical survey. Our main contribution of this paper is the current state of experimentation maturity as operationalized by the ExG model for a convenience sample of companies doing online controlled experiments. Our findings show that, among others, companies typically develop in-house experimentation platforms, that these platforms are of various levels of maturity, and that designing key metrics - Overall Evaluation Criteria - remains the key challenge for successful experimentation.","2018-08","2021-01-21 11:11:52","2021-01-21 11:11:52","","68-72","","","","","","Online Controlled Experimentation at Scale","","","","","","","","","","","","IEEE Xplore","","","","","","","A-B testing maturity; controlled experimentation, A/B testing, empirical survey, Experimentation Growth Model; DP industry; Electromagnetic interference; evaluation criteria; ExG model; in-house experimentation platforms; Noise measurement; OCEs; online Controlled experimentation; online software industry; program testing; Software engineering; software product lines; software products","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)","","","","","","","","","","","","","","",""
"ITUK3DGU","conferencePaper","2018","Wang, J.; Goldberg, D.; Burke, P.; Bhoite, D.","Designing and Analyzing A/B Tests in an Online Marketplace","2018 IEEE International Conference on Data Mining Workshops (ICDMW)","","","10.1109/ICDMW.2018.00206","","Internet companies often run A/B tests to make sure product decisions are data-driven and credible. When conducting these experiments, it is a common practice to randomly assign customers into test/control groups and assume that the behavior of a customer is not dependent on the assignment of another customer. We see the necessity to verify this assumption given the complex dynamics inside an online marketplace. Our way is to embrace a regression model for experiment response and study if the interference between test and control make a statistically significant regressor. To minimize the interference we advocate for changing the randomization and develop a system in support of that. A category based strategy is explored in detail. It leads to a much smaller sample size so we further discuss the application of transformation and variance reduction techniques in the analysis. We report encouraging results by taking logarithm of the response metric and leveraging pre-experiment data as covariates in the regression. While the described work is for a specific scenario, we believe the design and analysis can be generalized and used at other places to achieve trustworthy online experimentation.","2018-11","2021-01-21 11:12:17","2021-01-21 11:12:17","","1447-1452","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2375-9259","","C:\Users\wouterkok\Zotero\storage\A2GGCWHW\8637527.html","","","A/B test, controlled experiment, design of experiment, interference effect, randomization strategy, poststratification; A/B tests; Automobiles; business data processing; category based strategy; Conferences; consumer behaviour; customer behaviour; data analysis; DP industry; Estimation; Feeds; IEEE merchandise; Interference; Internet; Internet companies; Measurement; online marketplace; program testing; randomization; regression analysis; regression model; statistical analysis; variance reduction techniques","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 IEEE International Conference on Data Mining Workshops (ICDMW)","","","","","","","","","","","","","","",""
"PITFJQJD","conferencePaper","2016","Dmitriev, P.; Frasca, B.; Gupta, S.; Kohavi, R.; Vaz, G.","Pitfalls of long-term online controlled experiments","2016 IEEE International Conference on Big Data (Big Data)","","","10.1109/BigData.2016.7840744","","Online controlled experiments (e.g., A/B tests) are now regularly used to guide product development and accelerate innovation in software. Product ideas are evaluated as scientific hypotheses, and tested on web sites, mobile applications, desktop applications, services, and operating system features. One of the key challenges for organizations that run controlled experiments is to select an Overall Evaluation Criterion (OEC), i.e., the criterion by which to evaluate the different variants. The difficulty is that short-term changes to metrics may not predict the long-term impact of a change. For example, raising prices likely increases short-term revenue but also likely reduces long-term revenue (customer lifetime value) as users abandon. Degrading search results in a Search Engine causes users to search more, thus increasing query share short-term, but increasing abandonment and thus reducing longterm customer lifetime value. Ideally, an OEC is based on metrics in a short-term experiment that are good predictors of long-term value. To assess long-term impact, one approach is to run longterm controlled experiments and assume that long-term effects are represented by observed metrics. In this paper we share several examples of long-term experiments and the pitfalls associated with running them. We discuss cookie stability, survivorship bias, selection bias, and perceived trends, and share methodologies that can be used to partially address some of these issues. While there is clearly value in evaluating long-term trends, experimenters running long-term experiments must be cautious, as results may be due to the above pitfalls more than the true delta between the Treatment and Control. We hope our real examples and analyses will sensitize readers to the issues and encourage the development of new methodologies for this important problem.","2016-12","2021-01-21 11:12:25","2021-01-21 11:12:25","","1367-1376","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","C:\Users\wouterkok\Zotero\storage\98GNPYQ9\7840744.html","","","A/B testing; Browsers; Controlled experiments; cookie stability; Degradation; Google; Internet; long-term online controlled experiments; longterm customer lifetime value; Market research; Measurement; OEC; Online experiments; overall evaluation criterion; scientific hypotheses; search engine; search engines; selection bias; software engineering; software product development; survivorship bias; Testing; Web sites","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2016 IEEE International Conference on Big Data (Big Data)","","","","","","","","","","","","","","",""