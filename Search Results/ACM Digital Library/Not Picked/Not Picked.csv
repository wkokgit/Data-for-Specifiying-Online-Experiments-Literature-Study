"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"5XELBE8R","journalArticle","2019","Gupta, Somit; Kohavi, Ronny; Tang, Diane; Xu, Ya; Andersen, Reid; Bakshy, Eytan; Cardin, Niall; Chandran, Sumita; Chen, Nanyu; Coey, Dominic; Curtis, Mike; Deng, Alex; Duan, Weitao; Forbes, Peter; Frasca, Brian; Guy, Tommy; Imbens, Guido W.; Saint Jacques, Guillaume; Kantawala, Pranav; Katsev, Ilya; Katzwer, Moshe; Konutgan, Mikael; Kunakova, Elena; Lee, Minyong; Lee, MJ; Liu, Joseph; McQueen, James; Najmi, Amir; Smith, Brent; Trehan, Vivek; Vermeer, Lukas; Walker, Toby; Wong, Jeffrey; Yashkov, Igor","Top Challenges from the first Practical Online Controlled Experiments Summit","ACM SIGKDD Explorations Newsletter","","1931-0145","10.1145/3331651.3331655","https://doi.org/10.1145/3331651.3331655","Online controlled experiments (OCEs), also known as A/B tests, have become ubiquitous in evaluating the impact of changes made to software products and services. While the concept of online controlled experiments is simple, there are many practical challenges in running OCEs at scale. To understand the top practical challenges in running OCEs at scale and encourage further academic and industrial exploration, representatives with experience in large-scale experimentation from thirteen different organizations (Airbnb, Amazon, Booking.com, Facebook, Google, LinkedIn, Lyft, Microsoft, Netflix, Twitter, Uber, Yandex, and Stanford University) were invited to the first Practical Online Controlled Experiments Summit. All thirteen organizations sent representatives. Together these organizations have tested more than one hundred thousand experiment treatments last year. Thirty-four experts from these organizations participated in the summit in Sunnyvale, CA, USA on December 13-14, 2018. While there are papers from individual organizations on some of the challenges and pitfalls in running OCEs at scale, this is the first paper to provide the top challenges faced across the industry for running OCEs at scale and some common solutions.","2019-05-13","2021-01-22 10:16:20","2021-01-22 10:16:20","2021-01-22 10:16:03","20–35","","1","21","","SIGKDD Explor. Newsl.","","","","","","","","","","","","","June 2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XFIRVE69","conferencePaper","2020","Gupta, Somit; Shi, Xiaolin; Dmitriev, Pavel; Fu, Xin; Mukherjee, Avijit","Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments","Proceedings of the 13th International Conference on Web Search and Data Mining","978-1-4503-6822-3","","10.1145/3336191.3371871","https://doi.org/10.1145/3336191.3371871","A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the ""why"" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [11, 14]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 [23]. It was attended by around 150 participants.","2020-01-20","2021-01-22 10:16:20","2021-01-22 10:16:20","2021-01-22","877–880","","","","","","","WSDM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","a/b testing; controlled experiments; online metrics; user experience evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T92YF8YE","conferencePaper","2019","Shi, Xiaolin; Dmitriev, Pavel; Gupta, Somit; Fu, Xin","Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments","Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","978-1-4503-6201-6","","10.1145/3292500.3332297","https://doi.org/10.1145/3292500.3332297","A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the ""why"" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [18, 22]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions.","2019-07-25","2021-01-22 10:16:20","2021-01-22 10:16:20","2021-01-22","3189–3190","","","","","","","KDD '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","a/b testing; controlled experiments; online metrics; user experience evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CK6GSF8M","conferencePaper","2019","Fabijan, Aleksander; Dmitriev, Pavel; Olsson, Helena Holmström; Bosch, Jan; Vermeer, Lukas; Lewis, Dylan","Three key checklists and remedies for trustworthy analysis of online controlled experiments at scale","Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice","","","10.1109/ICSE-SEIP.2019.00009","https://doi.org/10.1109/ICSE-SEIP.2019.00009","Online Controlled Experiments (OCEs) are transforming the decision-making process of data-driven companies into an experimental laboratory. Despite their great power in identifying what customers actually value, experimentation is very sensitive to data loss, skipped checks, wrong designs, and many other 'hiccups' in the analysis process. For this purpose, experiment analysis has traditionally been done by experienced data analysts and scientists that closely monitored experiments throughout their lifecycle. Depending solely on scarce experts, however, is neither scalable nor bulletproof. To democratize experimentation, analysis should be streamlined and meticulously performed by engineers, managers, or others responsible for the development of a product. In this paper, based on synthesized experience of companies that run thousands of OCEs per year, we examined how experts inspect online experiments. We reveal that most of the experiment analysis happens before OCEs are even started, and we summarize the key analysis steps in three checklists. The value of the checklists is threefold. First, they can increase the accuracy of experiment se-tup and decision-making process. Second, checklists can enable novice data scientists and software engineers to become more autonomous in setting-up and analyzing experiments. Finally, they can serve as a base to develop trustworthy platforms and tools for OCE set-up and analysis.","2019-05-27","2021-01-22 10:16:20","2021-01-22 10:16:20","2021-01-22","1–10","","","","","","","ICSE-SEIP '19","","","","IEEE Press","Montreal, Quebec, Canada","","","","","","ACM Digital Library","","","","","","","a/b testing; experiment checklists; online controlled experiments","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"567GGHYP","conferencePaper","2019","Esteller-Cucala, Maria; Fernandez, Vicenc; Villuendas, Diego","Experimentation Pitfalls to Avoid in A/B Testing for Online Personalization","Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization","978-1-4503-6711-0","","10.1145/3314183.3323853","https://doi.org/10.1145/3314183.3323853","Online controlled experiments (also called A/B tests, bucket testing or randomized experiments) have become an habitual practice in numerous companies for measuring the impact of new features and changes deployed to softwares products. In theory, these experiments are one of the simplest methods to evaluate the potential effects that new features have on user's behavior. In practice, however, there are many pitfalls that can obscure the interpretation of results or induce invalid conclusions. There is, in the literature, no shortage of prior work on online controlled experiments addressing these pitfalls and conclusions misinterpretations, but the topic is not tackled considering the specific case of testing personalization features. In this paper, we present some of the experimentation pitfalls that are particularly important for personalization features. To better illustrate each pitfall, we include a combination of theoretical argumentation as well as examples from real company's experiments. While there is clearly value in evaluating personalized features by means of online controlled experiments, there are some pitfalls to bear in mind while testing. With this paper, we aim to increase the experimenters' awareness of leading to improved quality and reliability of the results.","2019-06-06","2021-01-22 10:16:20","2021-01-22 10:16:20","2021-01-22","153–159","","","","","","","UMAP'19 Adjunct","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","a/b testing; controlled experiments; online experiments; online personalization; personalization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""